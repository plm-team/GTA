

=== meta-llama/Llama-2-7b-hf nvidia_H100 w_bit=16 a_bit=16 kv_bit=16 batchsize=1 seqlen=2048 tp_size=1 ===
layer_name,OPs,Access,arithmetic_intensity,performance,bound,load_weight,load_act,store_act,load_kv_cache,store_kv_cache,inference_time
q_proj,68.7G,67.1MB,1.0K,990T,compute,33.6MB,16.8MB,16.8MB,0.00B,0.00B,69.4us
k_proj,68.7G,67.1MB,1.0K,990T,compute,33.6MB,16.8MB,0.00B,0.00B,16.8MB,69.4us
v_proj,68.7G,67.1MB,1.0K,990T,compute,33.6MB,16.8MB,0.00B,0.00B,16.8MB,69.4us
out_proj,68.7G,67.1MB,1.0K,990T,compute,33.6MB,16.8MB,16.8MB,0.00B,0.00B,69.4us
gate_proj,185G,152MB,1.2K,990T,compute,90.2MB,16.8MB,45.1MB,0.00B,0.00B,186.6us
up_proj,185G,152MB,1.2K,990T,compute,90.2MB,16.8MB,45.1MB,0.00B,0.00B,186.6us
down_proj,185G,152MB,1.2K,990T,compute,90.2MB,45.1MB,16.8MB,0.00B,0.00B,186.6us
qk_matmul,34.4G,302MB,113.8,350T,memory,0.00B,16.8MB,268MB,16.8MB,0.00B,98.3us
sv_matmul,34.4G,302MB,113.8,350T,memory,0.00B,268MB,16.8MB,16.8MB,0.00B,98.3us
softmax,671M,537MB,1.2,3.8T,memory,0.00B,268MB,268MB,0.00B,0.00B,174.8us
attn_norm,58.7M,33.6MB,1.8,5.4T,memory,0.00B,16.8MB,16.8MB,0.00B,0.00B,10.9us
mlp_norm,58.7M,33.6MB,1.8,5.4T,memory,0.00B,16.8MB,16.8MB,0.00B,0.00B,10.9us
attn_add,8.4M,33.6MB,0.25,768G,memory,0.00B,16.8MB,16.8MB,0.00B,0.00B,10.9us
mlp_add,8.4M,33.6MB,0.25,768G,memory,0.00B,16.8MB,16.8MB,0.00B,0.00B,10.9us
mlp_act,16.8M,50.3MB,0.33,1.0T,memory,0.00B,33.6MB,16.8MB,0.00B,0.00B,16.4us
lm_head,131M,262MB,0.50,1.5T,memory,262MB,8.2KB,64.0KB,0.00B,0.00B,85.4us
