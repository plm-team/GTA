

=== ../Llama3.2-1B nvidia_H100 w_bit=16 a_bit=16 kv_bit=16 batchsize=1 seqlen=2048 tp_size=1 ===
layer_name,OPs,memory_access,load_weight,load_act,store_act,load_kv_cache,store_kv_cache,inference_time,memory_consumption,memory_consumption_tmp_act,memory_consumption_weight,memory_consumption_kv_cache
decode,2.7G,2.4G,2.3G,11.5M,11122176.0,142M,69.1K,2.1G,201K,1.9G,142M,788.2us
prefill,5.1T,48.2G,2.3G,23.4G,22253136384.0,142M,142M,2.5G,412M,1.9G,142M,17.5ms


=== ../Llama3.2-1B nvidia_H100 w_bit=16 a_bit=16 kv_bit=16 batchsize=1 seqlen=1024 tp_size=1 ===
layer_name,OPs,memory_access,load_weight,load_act,store_act,load_kv_cache,store_kv_cache,inference_time,memory_consumption,memory_consumption_tmp_act,memory_consumption_weight,memory_consumption_kv_cache
decode,2.4G,2.3G,2.3G,7.1M,6698496.0,70.8M,69.1K,2.0G,119K,1.9G,70.8M,762.3us
prefill,2.3T,16.2G,2.3G,7.2G,6596848128.0,70.8M,70.8M,2.1G,122M,1.9G,70.8M,5.9ms


=== ../Llama3.2-1B nvidia_H100 w_bit=16 a_bit=16 kv_bit=16 batchsize=1 seqlen=512 tp_size=1 ===
layer_name,OPs,memory_access,load_weight,load_act,store_act,load_kv_cache,store_kv_cache,inference_time,memory_consumption,memory_consumption_tmp_act,memory_consumption_weight,memory_consumption_kv_cache
decode,2.2G,2.3G,2.3G,4.9M,4486656.0,35.4M,69.1K,2.0G,78.3K,1.9G,35.4M,749.3us
prefill,1.1T,6.9G,2.3G,2.5G,2166090240.0,35.4M,35.4M,2.0G,40.1M,1.9G,35.4M,2.3ms


=== ../Llama3.2-1B nvidia_H100 w_bit=16 a_bit=16 kv_bit=16 batchsize=1 seqlen=128 tp_size=1 ===
layer_name,OPs,memory_access,load_weight,load_act,store_act,load_kv_cache,store_kv_cache,inference_time,memory_consumption,memory_consumption_tmp_act,memory_consumption_weight,memory_consumption_kv_cache
decode,2.1G,2.3G,2.3G,3.2M,2827776.0,8.8M,69.1K,1.9G,47.6K,1.9G,8.8M,739.6us
prefill,252G,3.0G,2.3G,401M,329378304.0,8.8M,8.8M,1.9G,6.1M,1.9G,8.8M,978.2us


=== ../Llama3.2-1B nvidia_H100 w_bit=16 a_bit=16 kv_bit=16 batchsize=32 seqlen=2048 tp_size=1 ===
layer_name,OPs,memory_access,load_weight,load_act,store_act,load_kv_cache,store_kv_cache,inference_time,memory_consumption,memory_consumption_tmp_act,memory_consumption_weight,memory_consumption_kv_cache
decode,85.5G,7.5G,2.3G,369M,347957760.0,4.5G,2.2M,6.5G,6.4M,1.9G,4.5G,2.4ms
prefill,164T,1.5T,2.3G,749G,712092412416.0,4.5G,4.5G,19.6G,13.2G,1.9G,4.5G,555.1ms


=== ../Llama3.2-1B nvidia_H100 w_bit=16 a_bit=16 kv_bit=16 batchsize=32 seqlen=1024 tp_size=1 ===
layer_name,OPs,memory_access,load_weight,load_act,store_act,load_kv_cache,store_kv_cache,inference_time,memory_consumption,memory_consumption_tmp_act,memory_consumption_weight,memory_consumption_kv_cache
decode,76.3G,5.0G,2.3G,227M,206400000.0,2.3G,2.2M,4.2G,3.8M,1.9G,2.3G,1.6ms
prefill,72.7T,447G,2.3G,229G,211091188224.0,2.3G,2.3G,8.1G,3.9G,1.9G,2.3G,183.2ms


=== ../Llama3.2-1B nvidia_H100 w_bit=16 a_bit=16 kv_bit=16 batchsize=32 seqlen=512 tp_size=1 ===
layer_name,OPs,memory_access,load_weight,load_act,store_act,load_kv_cache,store_kv_cache,inference_time,memory_consumption,memory_consumption_tmp_act,memory_consumption_weight,memory_consumption_kv_cache
decode,71.6G,3.7G,2.3G,157M,135621120.0,1.1G,2.2M,3.1G,2.5M,1.9G,1.1G,1.2ms
prefill,34.0T,152G,2.3G,78.5G,69306935808.0,1.1G,1.1G,4.3G,1.3G,1.9G,1.1G,68.1ms


=== ../Llama3.2-1B nvidia_H100 w_bit=16 a_bit=16 kv_bit=16 batchsize=32 seqlen=128 tp_size=1 ===
layer_name,OPs,memory_access,load_weight,load_act,store_act,load_kv_cache,store_kv_cache,inference_time,memory_consumption,memory_consumption_tmp_act,memory_consumption_weight,memory_consumption_kv_cache
decode,68.2G,2.7G,2.3G,104M,82536960.0,283M,2.2M,2.2G,1.5M,1.9G,283M,888.2us
prefill,8.1T,26.2G,2.3G,12.8G,10532153856.0,283M,283M,2.4G,195M,1.9G,283M,12.7ms
